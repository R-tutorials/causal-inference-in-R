# Expressing causal questions as DAGs {#sec-dags}

{{< include 00-setup.qmd >}}

## Visualizing Causal Assumptions

So you think correlation isn't causation?
You better be ready to name some proposed confounders!
In @sec-assump we discuss several assumptions that allow us to estimate unbiased causal effects with our current statistical tools; one of the main assumptions is *exchangeability*, also known as "no unmeasured confounders".
There is no statistical test that can confirm whether this assumption is met.
Instead, we often use domain knowledge to construct an assumed world view of how different measured (or unmeasured) factors interact, and then *assuming that world view is correct* determine whether the proposed analysis included any unmeasured confounders.
One way to communicate one's world view with respect to how different factors interact is via a directed acyclic graph (DAG).

::: callout-tip
## Jargon

Why is it called a *directed acyclic graph*?
Let's start with the third word: **graph**.
A graph, as defined here, is a collection of *nodes* (sometimes these are called *vertices*) and *edges* that connect the nodes.
In mathematics, there is a whole field called *graph theory* which studies these graphs.
For our use, each *node* is a variable or factor, for example the exposure would be a node and likewise the outcome would be a node.
Edges between nodes are depicted with *arrows* that imply causality.
So if we think the exposure causes the outcome we would draw an arrow from the exposure to the outcome.
This one way connection between nodes (*from* causes *to* effects) makes this graph **directed**.
Finally, **acyclic** refers to the fact that there are no cycles or loops in the graph.
This makes sense because when thinking about causes and effects, loops are not possible without breaking the space-time continuum.
Often when a DAG erroneously includes a "loop" it is because the analyst did not appropriately consider the timing of the factors in question.
:::

For example, @fig-dag-ex, adapted from @mcgowan2023causal shows a sample DAG that suggests that `cause` causes `effect` and `other cause` causes both cause and effect.

```{r}
#| echo: false
#| label: fig-dag-ex
#| message: false
#| warning: false
#| fig-cap: "Example DAG adapted from @mcgowan2023causal. Here, there are three nodes representing three factors: `cause`, `other cause`, and `effect`. The arrows demonstrate the causal relationships between these factors such that `cause` causes `effect` and `other cause` causes both `cause` and `effect`."
library(ggdag)
library(ggokabeito)

coords <- list(
  x = c(cause = 2, Z = 1, effect = 3),
  y = c(cause = 1, Z = 1.1, effect = 1)
)
d_conf <- dagify(
  cause ~ Z,
  effect ~ cause + Z,
  exposure = "cause",
  outcome = "effect",
  labels = c(cause = "cause", effect = "effect", Z = "Z"),
  coords = coords
)

d_conf |>
  tidy_dagitty() |>
  mutate(name = ifelse(name == "Z", "other\ncause", name)) |>
  ggplot(
    aes(x = x, y = y, xend = xend, yend = yend)
  ) +
  geom_dag_point(aes(color = label)) +
  geom_dag_edges() +
  geom_dag_text(size = 3) +
  theme_dag() +
  coord_cartesian(clip = "off") +
  theme(legend.position = "none")
```

Thinking about any three nodes in a DAG, there are three ways they can be connected, via a *fork*, a *chain*, or a *collider*.
Examining @fig-dag-3, the fork describes the scenario where the `q` node has two arrows extending from it, one pointing to `x` and one pointing to `y`.
This implies that `q` causes both `x` and `y`.
If `x` were an exposure and `y` and outcome, `q` would be a classic *confounder*.
If we were trying to quantify the causal relationship between `x` and `y`, the forking caused by `q` provides a potential *backdoor path* from `x` to `y` that could lead to a spurious estimate of the relationship between `x` and `y` if not accounted for (sometimes we refer to this "accounting" as closing the backdoor path).
Associations can flow through forks.
The second panel of @fig-dag-3 in @fig-dag-3 displays a chain.
Here, the `x` node has an arrow to the `q` node which in turn has an arrow to the `y` node.
If `x` were an exposure and `y` and outcome, `q` would be a classic *mediator*.
In the final panel of @fig-dag-3, we see the collider.
Here `y` has an arrow to `q` and `x` has an arrow to `q`.
Colliders *block* backdoor paths.
Opposite from confounders, adjusting for a collider can actually *open* a backdoor path.

```{r}
#| echo: false
#| fig-width: 10
#| label: fig-dag-3
#| fig-cap: "Three types of paths connecting three nodes, `x`, `y`, and `q`: a fork, a chain, and a collider."
coords <- list(x = c(x = 0, y = 2, q = 1), y = c(x = 0, y = 0, q = 1))

fork <- dagify(
  x ~ q,
  y ~ q,
  exposure = "x",
  outcome = "y",
  coords = coords
)

chain <- dagify(
  q ~ x,
  y ~ q,
  exposure = "x",
  outcome = "y",
  coords = coords
)

collider <- dagify(
  q ~ x + y,
  exposure = "x",
  outcome = "y",
  coords = coords
)

map(list(fork = fork, chain = chain, collider = collider), tidy_dagitty, layout = "time_ordered") |>
  map("data") |>
  list_rbind(names_to = "dag") |>
  mutate(dag = factor(dag, levels = c("fork", "chain", "collider"))) |>
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges(edge_width = 1) +
  geom_dag_point() +
  geom_dag_text() +
  facet_wrap(~dag) +
  expand_plot(
    expand_x = expansion(c(.1, .1)),
    expand_y = expansion(c(0.2, 0.2))
  ) +
  theme_dag()
```

Often, the goal in causal inference is to quantify the relationship between some exposure and some outcome.
One way this estimate can be biased is if we are actually reporting the *correlation* between these two factors rather than the *causal relationship*.
Thinking about these DAGs, spurious correlations can often be attributed to *open backdoor paths*, i.e. the relationship between other factors and our exposure and outcome of interest.
One way to think about these paths is to conceptualize associations *flowing* along the paths.
Without adjustment, associations flow through forks and chains, and are *blocked* by colliders.
With adjustment, the opposite is true, associations are blocked by forks and chains if the node in question is adjusted for, however an association *will* flow through a collider if it is adjusted for.

While not strictly necessary, we recommend that DAGs are *time-ordered* from left to right.
This helps the analyst ensure that they are not erroneously making assumptions that violate the space-time continuum (for example it is impossible for something in the future to cause something from the past).

How do we use these in practice?
The basic idea is:

1.  Specify your causal question
2.  Using domain knowledge:

-   Write all relevant variables as *nodes*
-   Draw causal pathways as arrows (*edges*)

## DAGs in R

Let's begin by specifying our causal question: Does listening to a comedy podcast the morning before an exam improve graduate students test scores?
We can diagram this using the method describe in @sec-diag (@fig-podcast).

```{r}
#| echo: false
#| fig-cap: "Does listening to a comedy podcast the morning before an exam improve graduate students test scores?"
#| fig-height: 2
#| label: fig-podcast

data <- data.frame(
  labels = c("comedy podcast", "test scores", "graduate students", "morning", "graduate students", "after exam"),
  x = c(1.25, 1.75, 1.25, 1.55, 1.8, 2.05),
  y = c(1, 1, 0.8, 0.7, 0.8, 0.7),
  angle = c(0, 0, -30, 0, -30, 0)
)

ggplot(data, aes(x = x, y = y)) +
  geom_text(aes(label = labels, angle = angle, vjust = 0),
    size = 5
  ) +
  geom_segment(aes(x = 1, xend = 2, y = 0.95, yend = 0.95)) +
  geom_segment(aes(x = 1.5, xend = 1.5, y = 0.95, yend = 1.1)) +
  geom_segment(aes(x = 1, xend = 1.35, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1.35, xend = 1.65, y = 0.65, yend = 0.65)) +
  geom_segment(aes(x = 1.55, xend = 1.9, y = 0.95, yend = 0.65)) +
  geom_segment(aes(x = 1.9, xend = 2.15, y = 0.65, yend = 0.65)) +
  theme_void()
```

The first step is to specify your DAG.
In R, we can use the {ggdag} package for this along with the `dagify()` function.
The `dagify()` function takes formulas, separated by commas, that specify cause and effect, with the left element of of the formula specifying the effect and the right all of the factors that cause it.
What are all of the factors that "cause" graduate students to listen to a podcast the morning before an exam?
What are all of the factors that could "cause" a graduate student to do well on a test?
Let's posit some here.

```{r}
library(ggdag)
dagify(
  podcast ~ mood + humor + prepared,
  exam ~ mood + prepared
)
```

In the code chunk above, we have posited that a graduate students mood, sense of humor, and how prepared they feel for the exam could influence whether they listened to a podcast the morning of the text.
Likewise, we posit that their mood and how prepared they are also influences their exam score.
Notice we *do not* see `podcast` in the `exam` equation -- this means that we assume that there is no causal relationship between podcast and the exam score.
We can add additional arguments to `dagify()`, for example, we can time order the coordinates, tag the exposure and outcome, and add labels.
We can save this `dagify` object and use the `ggdag()` function to visualize this DAG.
This function is a wrapper for a `ggplot2` call, meaning we can add layers like we would to a ggplot object.
For example, we can update the theme by adding `+ theme_dag()`.

```{r}
#| label: fig-dag-podcast
#| fig-cap: "Proposed DAG to answer the question: Does listening to a comedy podcast the morning before an exam improve graduate students test scores?"
podcast_dag <- dagify(
  podcast ~ mood + humor + prepared,
  exam ~ mood + prepared,
  coords = time_ordered_coords(),
  exposure = "podcast",
  outcome = "exam",
  labels = c(
    podcast = "podcast",
    exam = "exam score",
    mood = "mood",
    humor = "humor",
    prepared = "prepared"
  )
)
ggdag(podcast_dag, use_labels = "label", text = FALSE) +
  theme_dag()
```

In the previous section, we discussed *backdoor paths*.
These are paths between factors that could potentially lead us to drawing spurious conclusions about the relationship between our exposure and outcome.
The `ggdag_paths()` function will help us identify potential backdoor paths.

```{r}
#| label: fig-paths
ggdag_paths(podcast_dag)
```

In @fig-paths we see two open paths, one through `mood` and one through `prepared`.
This tells us we need to find a way to account for these open, non-causal paths.
Some ways to do this include:

-   Randomization
-   Stratification, adjustment, weighting, matching, etc.

In this particular scenario, randomization is likely not possible.
It would be challenging to justify randomizing students to listening to a full podcast prior to taking an exam (and it would likely have lots of issues with adherence!).
Stratification is similar to what we demonstrated in @sec-group-sum.
We could stratify the students into all possible mood and prepared categories and analyze the causal effect within each stratum -- again, this might pose challenges depending on the sample size and the number of categories we believe exists in each of these factors.
The next section will dive into some of these other tools we could use to account for these potential backdoor paths.

The {ggdag} package can also help us identify adjustment sets.
In this particular example, this yields the same result as above, since we need to adjust for both `mood` and `prepared` in order to close the backdoor paths.
It is possible, however, to have different ways to close backdoor paths depending on the number of factors and complexity causal relationships assumed.

```{r}
ggdag_adjustment_set(
  podcast_dag,
  use_labels = "label",
  text = FALSE
)
```

Using our proposed DAG, let's simulate some data to see how this might occur in practice!

```{r}
set.seed(10)
sim_data <- podcast_dag |>
  simulate_data()
```

```{r}
sim_data
```

Since we have simulated this data, we know that this is a case where *standard methods will succeed* (see @sec-standard), and therefore can estimate the causal effect using a basic linear regression model.
@fig-dag-sim shows a forest plot of the simulated data based on our DAG.
Notice the model that only included the exposure resulted in a spurious effect (an estimate of -0.1 when we know the truth is 0), whereas the model that adjusted for the two variables as suggested by `ggdag_adjustment_set()` is not spurious (0.0).

```{r}
#| label: fig-dag-sim
#| fig-cap: "Forest plot of simulated data based on the DAG described in @fig-dag-podcast"
## Model that does not close backdoor paths
unadjusted_model <- lm(exam ~ podcast, sim_data) |>
  broom::tidy(conf.int = TRUE) |>
  dplyr::filter(term == "podcast") |>
  mutate(formula = "podcast")

## Model that closes backdoor paths
adjusted_model <- lm(exam ~ podcast + mood + prepared, sim_data) |>
  broom::tidy(conf.int = TRUE) |>
  dplyr::filter(term == "podcast") |>
  mutate(formula = "podcast + mood + prepared")

bind_rows(
  unadjusted_model,
  adjusted_model
) |>
  ggplot(aes(x = estimate, y = formula, xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 0, linewidth = 1, color = "grey80") +
  geom_pointrange(fatten = 3, size = 1) +
  theme_minimal(18) +
  labs(
    y = NULL,
    caption = "correct effect size: 0"
  )
```

<!-- ## Common Structures of Bias -->

<!-- ## Causal Inference is not (just) a statistical problem {#sec-quartets} -->

<!-- ## Causal and Predictive Models, Revisited {#sec-causal-pred-revisit} -->
