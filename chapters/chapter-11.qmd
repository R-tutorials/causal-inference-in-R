# Causal estimands {#sec-estimands}

{{< include 00-setup.qmd >}}

## Estimands, Estimators, Estimates

When analyzing to make causal inferences, we need to keep the causal question that we're interested in close to our chest.
The causal question helps guide not just the assumptions we need to make but also how we will go about answering the question.
This chapter deals with three ideas closely related to answering causal questions: the estimand, the estimator, and the estimate.
The **estimand** is the *target of interest*, the **estimator** is the method by which we approximate this estimand using data, and the **estimate** is the value we get when we plug our data into the estimator.
You can think of the **estimand** as a glossy picture of a beautiful cake we are trying to bake, the **estimator** as the recipe, and the **estimate** as the cake we pull out of our oven.

So far, we have been estimating the average treatment effect, the effect of the exposure of interest averaged across the whole population.
The **estimand** here is the expected value of the difference in potential outcomes across all individuals:

$$E[Y(1) - Y(0)]$$

The **estimator** we use depends on the method we've chosen.
For example, in an A/B test or randomized controlled trial, our estimator could just be the average outcome among those who received the exposure A minus the average outcome among those who receive exposure B.

$$\sum_{i=1}^{N}\frac{Y_i\times X_i}{N_{\textrm{A}}} - \frac{Y_i\times (1-X_i)}{N_{\textrm{B}}}$$

Where $X$ is just an indicator for exposure A ($X = 1$ for exposure A and $X = 0$ for exposure B), $N_A$ is the total number in group A and $N_B$ is the total number in group B such that $N_A + N_B = N$.
Let's motivate this example a bit more.
Suppose we have two different designs for a call-to-action (often abbreviated as CTA, a written directive for a marketing campaign) and want to assess whether they lead to a different average number of items a user purchases.
We can create an A/B test where we randomly assign a subset of users to see design A or design B.
Suppose we have A/B testing data for 100 participants in a dataset called `ab`.
Here is some code to simulate such a data set.

```{r}
#| eval: false
library(tidyverse)
set.seed(928)
ab <- tibble(
  # generate the exposure, x, from a binomial distribution
  # with the probability exposure A of 0.5
  x = rbinom(100, 1, 0.5),
  # generate the "true" average treatment effect of 1
  # we use rnorm(100) to add the random error term that is normally
  # distributed with a mean of 0 and a standard deviation of 1
  y = x + rnorm(100)
)
```

Here the exposure is `x`, and the outcome is `y`.
The true average treatment effect is 1, implying that on average seeing call-to-action A increases the number of items purchased by 1.

```{r}
#| echo: false
set.seed(928)
ab <- tibble(
  x = rbinom(100, 1, 0.5),
  y = x + rnorm(100)
)
ab |>
  summarise(
    n_a = sum(x),
    n_b = sum(1 - x),
    estimate = sum(
      (y * x) / n_a -
        y * (1 - x) / n_b
    )
  ) |>
  pull(estimate) -> estimate
```

```{r}
ab
```

Below are two ways to estimate this in R.
Using a formula approach, we calculate the difference in `y` between exposure values in the first example.

```{r}
ab |>
  summarise(
    n_a = sum(x),
    n_b = sum(1 - x),
    estimate = sum(
      (y * x) / n_a -
        y * (1 - x) / n_b
    )
  )
```

Alternatively, we can `group_by()` `x` and `summarize()` the means of `y` for each group, then pivot the results to take their difference.

```{r}
ab |>
  group_by(x) |>
  summarise(avg_y = mean(y)) |>
  pivot_wider(
    names_from = x,
    values_from = avg_y,
    names_prefix = "x_"
  ) |>
  summarise(estimate = x_1 - x_0)
```

Because $X$, the exposure, was randomly assigned, this estimator results in an unbiased estimate of our estimand of interest.

What do we mean by unbiased, though?
Notice that while the "true" causal effect is 1, this estimate is not precisely 1; it is `r round(estimate, 3)`.
Why the difference?
This A/B test included a sample of 100 participants, not the whole population.
As that sample increases, our estimate will get closer to the truth.
Let's try that.
Let's simulate the data again but increase the sample size from 100 to 100,000:

```{r}
set.seed(928)
ab <- tibble(
  x = rbinom(100000, 1, 0.5),
  y = x + rnorm(100000)
)

ab |>
  summarise(
    n_a = sum(x),
    n_b = sum(1 - x),
    estimate = sum(
      (y * x) / n_a -
        y * (1 - x) / n_b
    )
  )
```

Notice how the estimate is 1.01, much closer to the actual average treatment effect, 1.

Suppose $X$ had not been randomly assigned.
In that case, we could use the pre-exposure covariates to estimate the conditional probability of $X$ (the propensity score) and incorporate this probability in a *weight* $(w_i)$ to estimate this causal effect.
For example, we could use the following *estimator* to *estimate* our average treatment effect *estimand*:

$$\frac{\sum_{i=1}^NY_i\times X_i\times w_i}{\sum_{i=1}^NX_i\times w_i}-\frac{\sum_{i=1}^NY_i\times(1-X_i)\times w_i}{\sum_{i=1}^N(1-X_i)\times w_i}$$

## Estimating treatment effects with specific targets in mind

Depending on the study's goal, or the causal question, we may want to estimate different *estimands*.
Here, we will outline the most common causal estimands, their target populations, the causal questions they may help answer, and the methods used to estimate them [@greifer2021choosing].

### Average treatment effect

A common estimand is the average treatment effect (ATE).
The target population is the *total sample* or population of interest.
The **estimand** here is the expected value of the difference in potential outcomes across all individuals:

$$E[Y(1) - Y(0)]$$

An example research question is "Should a policy be applied to all eligible patients?" [@greifer2021choosing].

Most randomized controlled trials are designed with the ATE as the target estimand.
In a non-randomized setting, you can estimate the ATE using full matching.
Each observation in the exposed group is matched to at least one observation in the control group (and vice versa) without replacement.
Alternatively, the following inverse probability weight will allow you to estimate the ATE using propensity score weighting.

$$w_{ATE} = \frac{X}{p} + \frac{(1 - X)}{1 - p}$$

In other words, the weight is one over the propensity score for those in the exposure group and one over one minus the propensity score for the control group.
Intuitively, this weight equates to the inverse probability of being in the exposure group in which you actually were.

Let's dig deeper into this causal estimand using the Touring Plans data.
Recall that in @sec-using-ps, we examined the relationship between whether there were "Extra Magic Hours" in the morning and the average wait time for the Seven Dwarfs Mine Train the same day between 9 am and 10 am.
Let's reconstruct our data set, `seven_dwarfs` and fit the same propensity score model specified previously.

```{r}
library(broom)
library(touringplans)

seven_dwarfs <- seven_dwarfs_train_2018 |>
  filter(wait_hour == 9) |>
  mutate(park_extra_magic_morning = factor(
    park_extra_magic_morning,
    labels = c("No Magic Hours", "Extra Magic Hours")
  ))

seven_dwarfs_with_ps <- glm(
  park_extra_magic_morning ~ park_ticket_season + park_close + park_temperature_high,
  data = seven_dwarfs,
  family = binomial()
) |>
  augment(type.predict = "response", data = seven_dwarfs)
```

Let's examine a table of the variables of interest in this data frame.
To do so, we are going to use the `tbl_summary()` function from the `{gtsummary}` package.
(We'll also use the `{labelled}` package to clean up the variable names for the table.)

```{r}
#| label: tbl-unweighted-gtsummary
#| tbl-cap: A descriptive table of Extra Magic Morning in the touringplans dataset. This table shows the distributions of these variables in the observed population.
library(gtsummary)
library(labelled)
seven_dwarfs_with_ps <- seven_dwarfs_with_ps |>
  set_variable_labels(
    park_ticket_season = "Ticket Season",
    park_close = "Close Time",
    park_temperature_high = "Historic High Temperature"
  )

tbl_summary(
  seven_dwarfs_with_ps,
  by = park_extra_magic_morning,
  include = c(park_ticket_season, park_close, park_temperature_high)
) |>
  # add an overall column to the table
  add_overall(last = TRUE)
```

From @tbl-unweighted-gtsummary, 294 days did not have extra magic hours in the morning, and 60 did.
We also see that 30% of the extra magic mornings were during peak season compared to 20% of the non-extra magic mornings.
Additionally, days with extra magic mornings were more likely to close at 6 pm (18:00:00) compared to non-magic hour mornings.
The median high temperature on days with extra magic hour mornings is slightly lower (83 degrees) compared to non-extra magic hour morning days.

Now let's construct our propensity score weight to estimate the ATE.
The `{propensity}` package has helper functions to estimate weights.
The `wt_ate()` function will calculate ATE weights.

```{r}
library(propensity)
seven_dwarfs_wts <- seven_dwarfs_with_ps |>
  mutate(w_ate = wt_ate(.fitted, park_extra_magic_morning))
```

Let's look at a distribution of these weights.

```{r}
#| label: fig-sd-ate-hist
#| fig.cap: >
#|   A histogram of the average treatment effect (ATE) weights for whether or not a day had extra magic hours. ATE weights can range from 1 to infinity, so paying attention to the actual range of weights is important.
ggplot(seven_dwarfs_wts, aes(x = w_ate)) +
  geom_histogram(bins = 50)
```

Many weights in @fig-sd-ate-hist are close to 1 (the smallest possible value for an ATE weight), with a long tail leading to the largest weight (around 24).
This distribution highlights a potential problem with ATE weights.
They range from 1 to infinity; if your weights are too large in small samples, this can lead to bias in your estimates (known as finite sample bias).

::: {.callout-warning}
## Finite sample bias

We know that not accounting for confounders can bias our causal estimates, but it turns out that even after accounting for all confounders, we may still get a biased estimate in finite samples.
Many of the properties we tout in statistics rely on *large samples*---how "large" is defined can be opaque.
Let's look at a quick simulation.
Here, we have an exposure, $X$, an outcome, $Y$, and one confounder, $Z$.
We will simulate $Y$, which is only dependent on $Z$ (so the true treatment effect is 0), and $X$, which also depends on $Z$.

```{r}
set.seed(928)
n <- 100
finite_sample <- tibble(
  # z is normally distributed with a mean: 0 and sd: 1
  z = rnorm(n),
  # x is defined from a probit selection model with normally distributed errors
  x = case_when(
    0.5 + z + rnorm(n) > 0 ~ 1,
    TRUE ~ 0
  ),
  # y is continuous, dependent only on z with normally distrbuted errors
  y = z + rnorm(n)
)
```

If we fit a propensity score model using the one confounder $Z$ and calculate the weighted estimator, we should get an unbiased result (which in this case would be 0).

```{r}
## fit the propensity score model
finite_sample_wts <- glm(
  x ~ z,
  data = finite_sample, family = binomial("probit")
) |>
  augment(newdata = finite_sample, type.predict = "response") |>
  mutate(wts = wt_ate(.fitted, x))

finite_sample_wts |>
  summarize(
    effect = sum(y * x * wts) / sum(x * wts) -
      sum(y * (1 - x) * wts) / sum((1 - x) * wts)
  )
```

Ok, here, our effect of $X$ is pretty far from 0, although it's hard to know if this is really bias, or something we are just seeing by chance in this particular simulated sample.
To explore the potential for finite sample bias, we can rerun this simulation many times at different sample sizes.
Let's do that.

```{r}
#| label: sim-finite-sample-bias
#| warning: false
#| message: false
#| cache: true
sim <- function(n) {
  ## create a simulated dataset
  finite_sample <- tibble(
    z = rnorm(n),
    x = case_when(
      0.5 + z + rnorm(n) > 0 ~ 1,
      TRUE ~ 0
    ),
    y = z + rnorm(n)
  )
  finite_sample_wts <- glm(
    x ~ z,
    data = finite_sample, family = binomial("probit")
  ) |>
    augment(newdata = finite_sample, type.predict = "response") |>
    mutate(wts = wt_ate(.fitted, x))
  bias <- finite_sample_wts |>
    summarize(
      effect = sum(y * x * wts) / sum(x * wts) -
        sum(y * (1 - x) * wts) / sum((1 - x) * wts)
    ) |>
    pull()
  tibble(
    n = n,
    bias = bias
  )
}

## Examine 5 different sample sizes, simulate each 1000 times
set.seed(1)
finite_sample_sims <- map_df(rep(c(50, 100, 500, 1000, 5000, 10000),
  each = 1000
), sim)

bias <- finite_sample_sims %>%
  group_by(n) %>%
  summarise(bias = mean(bias))
```

Let's plot our results:

```{r}
#| label: fig-finite-sample-bias
#| fig-cap: "Finite sample bias present with ATE weights created using correctly specified propensity score model, varying the sample size from n = 50 to n = 10,000"
ggplot(bias, aes(x = n, y = bias)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0, lty = 2)
```

This is an example of finite sample bias.
Notice here, even when the sample size is quite large (5,000) we still see some bias away from the "true" effect of 0.
It isn't until a sample size larger than 10,000 that we see this bias disappear.

Estimands that utilize weights that are unbounded (i.e. that theoretically can be infinitely large) are more likely to suffer from finite sample bias.
The likelihood of falling into finite sample bias depends on:\
(1) the estimand you have chosen (i.e. are the weights bounded?)\
(2) the distribution of the covariates in the exposed and unexposed groups (i.e. is there good overlap? Potential positivity violations, when there is poor overlap, are the regions where weights can become too large)\
(3) the sample size.
:::

The `{gtsummary}` package allows the creation of *weighted* tables, which can help us build an intuition for the psuedopopulation created by the weights.
First, we must load the `{survey}` package and create a survey design object.
The `{survey}` package supports calculating statistics and models using weights.
Historically, many researchers applied techniques incorporating weights to survey analyses.
Even though we are not doing a survey analysis, the same techniques are helpful for our propensity score weights.

```{r}
#| label: tbl-weighted-gtsummary
#| tbl-cap: A descriptive table of Extra Magic Morning hours weighted by the Average Treatment Effect Weights. This table shows the distributions of these variables in the psuedopopulation created by these weights.
#| message: false
library(survey)
seven_dwarfs_svy <- svydesign(
  ids = ~1,
  data = seven_dwarfs_wts,
  weights = ~w_ate
)
tbl_svysummary(
  seven_dwarfs_svy,
  by = park_extra_magic_morning,
  include = c(park_ticket_season, park_close, park_temperature_high)
) |>
  add_overall(last = TRUE)
```

Notice in @tbl-weighted-gtsummary that the variables are more **balanced** between the groups.
For example, looking at the variable `park_ticket_season`, in @tbl-unweighted-gtsummary we see that a higher percentage of the days with extra magic hours in the mornings were during the peak season (30%) compared to the percent of days without extra magic hours in the morning (23%).
In @tbl-weighted-gtsummary this is balanced, with a weighted percent of 22% peak days in the extra magic morning group and 22% in the no extra magic morning group.
The weights change the variables' distribution by weighting each observation so that the two groups appear balanced.
Also, notice that the distribution of variables in @tbl-weighted-gtsummary now matches the Overall column of @tbl-unweighted-gtsummary.
That is what ATE weights do: they make the distribution of variables included in the propensity score for each exposure group look like the overall distribution across the whole sample.

Let's look at another visualization that can help us understand the weighted psuedopopulation created by the ATE weights: a mirrored histogram.
We will plot the distribution of the propensity score for the days in the exposure group (days with extra magic hours in the morning) on the top half of the plot; then, we will mirror the distribution of the propensity score for the days in the"unexposed group below it. This visualization allows us to compare the two distributions quickly. Then we will overlay weighted histograms to demonstrate how these distributions differ after incorporating the weights. The {halfmoon} package includes a function `geom_mirror_histogram()` to assist with this visualization.

```{r}
#| label: fig-sd-mirror-hist-ate
#| fig.cap: >
#|   A mirrored histogram of the distribution of propensity scores between exposure groups. The dark bars represent the unweighted distribution, and the colored bars represent the distribution weighted by the average treatment effect (ATE) weight.
library(halfmoon)
ggplot(seven_dwarfs_wts, aes(.fitted, group = park_extra_magic_morning)) +
  geom_mirror_histogram(bins = 50) +
  geom_mirror_histogram(
    aes(fill = park_extra_magic_morning, weight = w_ate),
    bins = 50,
    alpha = 0.5
  ) +
  scale_y_continuous(labels = abs) +
  labs(
    x = "propensity score",
    fill = "Extra Magic Morning"
  )
```

We can learn a few things from @fig-sd-mirror-hist-ate.
First, we can see that there are more "unexposed" days---days without extra magic hours in the morning---compared to "exposed" in the observed population.
We can see this by examining the darker histograms, which show the distributions in the observed sample.
Similarly, the exposed days are upweighted more substantially, as evidenced by the light blue we see on the top.
We can also see that after weighting, the two distributions look similar; the shape of the blue weighted distribution on top looks comparable to the orange weighted distribution below.

### Average treatment effect among the treated

The target population to estimate the average treatment effect among the treated (ATT) is the *exposed* (treated) population.
This causal estimand conditions on those in the exposed group:

$$E[Y(1) - Y(0) | X = 1]$$

Example research questions where the ATT is of interest could include "Should we stop our marketing campaign to those currently receiving it?" or "Should medical providers stop recommending treatment for those currently receiving it?" [@greifer2021choosing]

The ATT is a common target estimand when using matching; here, all exposed observations are included and "matched" to control observations, some of which may be discarded.
Alternatively, the ATT can be estimated via weighting.
The ATT weight is estimated by:

$$w_{ATT} = X + \frac{(1 - X)p}{1 - p}$$

Let's add the ATT weights to the `seven_dwarfs_wts` data frame and look at their distribution.

```{r}
#| label: fig-sd-att-hist
#| fig.cap : >
#|   A histogram of the average treatment effect among the treated (ATT) weights. The range of the ATT weights in this example is more stable than the ATE weights: the range is much smaller.
seven_dwarfs_wts <- seven_dwarfs_wts |>
  mutate(w_att = wt_att(.fitted, park_extra_magic_morning))

ggplot(seven_dwarfs_wts, aes(w_att)) +
  geom_histogram(bins = 50)
```

Compared to the ATE weights, the weights in @fig-sd-att-hist look more *stable*; the distribution of the weights is not heavily skewed, and the range is small, from ground zero to a bit over one, with many at precisely one.
These weights are exactly one for all days in the exposed group.
Theoretically, for unexposed days these weights can range from zero to infinity.
Still, because we have many more unexposed days compared to exposed in this particular sample, the range is much smaller, meaning the vast majority of unexposed days are "down-weighted" to match the variable distribution of the exposed days.
Let's take a look at the weighted table to see this a bit more clearly.

```{r}
#| label: tbl-weighted-att
#| tbl-cap: A descriptive table of Extra Magic Morning hours weighted by the Average Treatment Effect among the Treated Weights. This table shows the distributions of these variables in the psuedopopulation created by these weights.
#| message: false
seven_dwarfs_svy <- svydesign(
  ids = ~1,
  data = seven_dwarfs_wts,
  weights = ~w_att
)
tbl_svysummary(
  seven_dwarfs_svy,
  by = park_extra_magic_morning,
  include = c(park_ticket_season, park_close, park_temperature_high)
) |>
  add_overall(last = TRUE)
```

We again achieve a balance between groups, but the target values in @tbl-weighted-att differ from the previous tables.
Recall in our ATE weighted table (@tbl-weighted-gtsummary), when looking at the `wdw_ticket_season` variable, we saw the weighted percent in the peak season balanced close to 22%, the overall percent of peak days in the observed sample.
We again see balance, but the weighted percent of peak season days is 30% in the exposed and unexposed groups, reflecting the percent in the unweighted exposure group.
If you compare @tbl-weighted-att to our unweighted table (@tbl-unweighted-gtsummary), you might notice that the columns are most similar to the exposed column from the unweighted table.
This is because the "target" population is the exposed group, the days with extra magic hours in the morning, so we're trying to make the comparison group (no magic hours) as similar to that group as possible.

We can again create a mirrored histogram to observe the weighted psuedopopulation.

```{r}
#| label: fig-sd-mirror-hist-att
#| fig.cap: >
#|   A mirrored histogram of the distribution of propensity scores between exposure groups. The dark bars represent the unweighted distribution, and the colored bars represent the distribution weighted by the average treatment effect among the treated (ATT) weight.
ggplot(seven_dwarfs_wts, aes(.fitted, group = park_extra_magic_morning)) +
  geom_mirror_histogram(bins = 50) +
  geom_mirror_histogram(
    aes(fill = park_extra_magic_morning, weight = w_att),
    bins = 50,
    alpha = 0.5
  ) +
  scale_y_continuous(labels = abs) +
  labs(
    x = "propensity score",
    fill = "Extra Magic Morning"
  )
```

Since every day in the exposed group has a weight of 1, the distribution of the propensity scores (the dark bars above 0 on the graph) in @fig-sd-mirror-hist-att overlaps exactly with the weighted distribution overlaid in blue.
In the unexposed population, we see that almost all observations are *down weighted*; the dark orange distribution is smaller than the distribution of the propensity scores and more closely matches the exposed distribution above it.
The ATT is easier to estimate when there are many more observations in the unexposed group compared to the exposed one.

### Average treatment effect among the unexposed

The target population to estimate the average treatment effect among the unexposed (control) (ATU / ATC) is the *unexposed* (control) population.
This causal estimand conditions on those in the unexposed group.

$$E[Y(1) - Y(0) | X = 0]$$

Example research questions where the ATU is of interest could include "Should we send our marketing campaign to those not currently receiving it?" or "Should medical providers begin recommending treatment for those not currently receiving it?" [@greifer2021choosing]

The ATU can be estimated via matching; here, all unexposed observations are included and "matched" to exposed observations, some of which may be discarded.
Alternatively, the ATU can be estimated via weighting.
The ATU weight is estimated by:

$$w_{ATU} = \frac{X(1-p)}{p}+ (1-X)$$ Let's add the ATU weights to the `seven_dwarfs_wts` data frame and look at their distribution.

When fitting an outcome model on matched data sets, we can simply subset the original data to only those who were matched and then fit a model on these data as we would otherwise. For example, re-performing the matching as we did in @sec-using-ps, we can extract the matched observations in a dataset called `matched_data` as follows.

```{r}
#| message: false
#| warning: false
library(broom)
library(touringplans)
library(MatchIt)

seven_dwarfs_9 <- seven_dwarfs_train_2018 |> 
  filter(wait_hour == 9) 

m <- matchit(
  park_extra_magic_morning ~ park_ticket_season + park_close + park_temperature_high,
  data = seven_dwarfs_9
)
matched_data <- get_matches(m)
```

We can then fit the outcome model on this data. For this analysis, we are interested in the impact of extra magic morning hours on the average posted wait time between 9 and 10am. The linear model below will estimate this in the matched cohort.

```{r}
lm(wait_minutes_posted_avg ~ park_extra_magic_morning, data = matched_data) |>
  tidy(conf.int = TRUE)
```

Recall that by default `{MatchIt}` estimates an average treatment effect among the treated. This means among days that have extra magic hours, the expected impact of having extra magic hours on the average posted wait time between 9 and 10am is 7.9 minutes (95% CI: 1.2-14.5). 

## Using weights in outcome models

Now let's use propensity score weights to estimate this same estimand. We will use the ATT weights so the analysis matches that for matching above.

```{r}
#| message: false
#| warning: false
library(propensity)

seven_dwarfs_9_with_ps <-
  glm(
    park_extra_magic_morning ~ park_ticket_season + park_close + park_temperature_high,
    data = seven_dwarfs_9,
    family = binomial()
  ) |>
  augment(type.predict = "response", data = seven_dwarfs_9)
seven_dwarfs_9_with_wt <- seven_dwarfs_9_with_ps |>
  mutate(w_att = wt_att(.fitted, park_extra_magic_morning))
```

We can fit a *weighted* outcome model by using the `weights` argument. 

```{r}
lm(wait_minutes_posted_avg ~ park_extra_magic_morning, 
   data = seven_dwarfs_9_with_wt,
   weights = w_att) |>
  tidy()
```

Using weighting, we estimate that among days that have extra magic hours, the expected impact of having extra magic hours on the average posted wait time between 9 and 10am is 6.2 minutes. 
While this approach will get us the desired estimate for the point estimate, the default output using the `lm` function for the uncertainty (the standard errors and confidence intervals) are not correct.


## Estimating uncertainty

There are three ways to estimate the uncertainty:

1. A bootstrap
2. A sandwich estimator that only takes into account the outcome model
3. A sandwich estimator that takes into account the propensity score model

The first option can be computationally intensive, but should get you the correct estimates.
The second option is computationally the easiest, but tends to overestimate the variability. There are not many current solutions in R (aside from coding it up yourself) for the third; however, the `{PSW}` package will do this. 

### The bootstrap

1. Create a function to run your analysis once on a sample of your data

```{r}
fit_ipw <- function(split, ...) {
  .df <- analysis(split)
  
  # fit propensity score model
  propensity_model <- glm(
    park_extra_magic_morning ~ park_ticket_season + park_close + park_temperature_high,
    data = seven_dwarfs_9,
    family = binomial()
  ) 
  
  # calculate inverse probability weights
  .df <- propensity_model |>
    augment(type.predict = "response", data = .df) |>
    mutate(wts = wt_att(.fitted, 
                        park_extra_magic_morning,
                        exposure_type = "binary"))
  
  # fit correctly bootstrapped ipw model
  lm(wait_minutes_posted_avg ~ park_extra_magic_morning, 
     data = .df, weights = wts) |>
    tidy()
}
```


2. Use {rsample} to bootstrap our causal effect

```{r}
#| message: false
#| warning: false
library(rsample)

# fit ipw model to bootstrapped samples
bootstrapped_seven_dwarfs <- bootstraps(
  seven_dwarfs_9, 
  times = 1000, 
  apparent = TRUE
)

ipw_results <- bootstrapped_seven_dwarfs |> 
  mutate(boot_fits = map(splits, fit_ipw)) 

ipw_results
```


Let's look at the results.

```{r}
ipw_results |>
  mutate(
    estimate = map_dbl(
      boot_fits,
      \(.fit) .fit |>
        filter(term == "park_extra_magic_morning") |>
        pull(estimate)
    )
  ) |>
  ggplot(aes(estimate)) +
  geom_histogram(bins = 30, fill = "#D55E00FF", color = "white", alpha = 0.8) + 
  theme_minimal()
```


3. Pull out the causal effect

```{r}
# get t-based CIs
boot_estimate <- int_t(ipw_results, boot_fits) |> 
  filter(term == "park_extra_magic_morning")
boot_estimate
```

We estimate that among days that have extra magic hours, the expected impact of having extra magic hours on the average posted wait time between 9 and 10am is `r round(boot_estimate$.estimate, 1)` minutes, 95% CI (`r round(boot_estimate$.lower, 1)`, `r round(boot_estimate$.upper, 1)`). 

### The outcome model sandwich

There are two ways to get the sandwich estimator. The first is to use the same weighted outcome model as above along with the `{sandwich}` package. Using the `sandwich` function, we can get the robust estimate for the variance for the parameter of interest, as shown below.

```{r}
#| message: false
#| warning: false
library(sandwich)
weighted_mod <- lm(wait_minutes_posted_avg ~ park_extra_magic_morning, 
   data = seven_dwarfs_9_with_wt,
   weights = w_att) 

sandwich(weighted_mod)
```

Here, our robust variance estimate is `r round(sandwich(weighted_mod)[2,2], 3)`. We can then use this to construct a robust confidence interval.

```{r}
robust_var <- sandwich(weighted_mod)[2, 2]
point_est <- coef(weighted_mod)[2]
lb <- point_est - 1.96 * sqrt(robust_var)
ub <- point_est + 1.96 * sqrt(robust_var)
lb
ub
```
We estimate that among days that have extra magic hours, the expected impact of having extra magic hours on the average posted wait time between 9 and 10am is `r round(point_est, 1)` minutes, 95% CI (`r round(lb, 1)`, `r round(ub, 1)`). 

Alternatively, we could fit the model using the `{survey}` package. To do this, we need to create a design object, like we did when fitting weighted tables.

```{r}
#| message: false
#| warning: false
library(survey)

des <- svydesign(
  ids = ~1,
  weights = ~w_att,
  data = seven_dwarfs_9_with_wt
)
```

Then we can use `svyglm` to fit the outcome model.

```{r}
svyglm(wait_minutes_posted_avg ~ park_extra_magic_morning, des) |>
  tidy(conf.int = TRUE)
```
### Sandwich estimator that takes into account the propensity score model

The correct sandwich estimator will also take into account the propensity score model. The `{PSW}` will allow us to do this. This package has some quirks, for example it doesn't work well with categorical variables, so we need to create dummy variables for `park_ticket_season` to pass into the model. *Actually, the code below isn't working because it seems there is a bug in the package. Stay tuned!*

```{r}
#| eval: false
library(PSW)

seven_dwarfs_9 <- seven_dwarfs_9 |>
  mutate(park_ticket_season_regular = ifelse(park_ticket_season == "regular", 1, 0),
         park_ticket_season_value = ifelse(park_ticket_season == "value", 1, 0)
  )
psw(data = seven_dwarfs_9, 
    form.ps = "park_extra_magic_morning ~ park_ticket_season_regular + park_ticket_season_value + park_close + park_temperature_high",
    weight = "ATT",
    wt = TRUE,
    out.var = "wait_minutes_posted_avg")
```

