{
  "hash": "fb7b3693149671381024a90b5463dc2e",
  "result": {
    "markdown": "# Expressing causal questions as DAGs {#sec-dags}\n\n\n\n\n\n## Visualizing Causal Assumptions\n\nSo you think correlation isn't causation?\nYou better be ready to name some proposed confounders!\nIn @sec-assump we discuss several assumptions that allow us to estimate unbiased causal effects with our current statistical tools; one of the main assumptions is *exchangeability*, also known as \"no unmeasured confounders\".\nThere is no statistical test that can confirm whether this assumption is met.\nInstead, we often use domain knowledge to construct an assumed world view of how different measured (or unmeasured) factors interact, and then *assuming that world view is correct* determine whether the proposed analysis included any unmeasured confounders.\nOne way to communicate one's world view with respect to how different factors interact is via a directed acyclic graph (DAG).\n\n::: callout-tip\n## Jargon\n\nWhy is it called a *directed acyclic graph*?\nLet's start with the third word: **graph**.\nA graph, as defined here, is a collection of *nodes* (sometimes these are called *vertices*) and *edges* that connect the nodes.\nIn mathematics, there is a whole field called *graph theory* which studies these graphs.\nFor our use, each *node* is a variable or factor, for example the exposure would be a node and likewise the outcome would be a node.\nEdges between nodes are depicted with *arrows* that imply causality.\nSo if we think the exposure causes the outcome we would draw an arrow from the exposure to the outcome.\nThis one way connection between nodes (*from* causes *to* effects) makes this graph **directed**.\nFinally, **acyclic** refers to the fact that there are no cycles or loops in the graph.\nThis makes sense because when thinking about causes and effects, loops are not possible without breaking the space-time continuum.\nOften when a DAG erroneously includes a \"loop\" it is because the analyst did not appropriately consider the timing of the factors in question.\n:::\n\nFor example, @fig-dag-ex, adapted from @mcgowan2023causal shows a sample DAG that suggests that `cause` causes `effect` and `other cause` causes both cause and effect.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Example DAG adapted from @mcgowan2023causal. Here, there are three nodes representing three factors: `cause`, `other cause`, and `effect`. The arrows demonstrate the causal relationships between these factors such that `cause` causes `effect` and `other cause` causes both `cause` and `effect`.](chapter-05_files/figure-html/fig-dag-ex-1.png){#fig-dag-ex width=672}\n:::\n:::\n\n\nThinking about any three nodes in a DAG, there are three ways they can be connected, via a *fork*, a *chain*, or a *collider*.\nExamining @fig-dag-3, the fork describes the scenario where the `q` node has two arrows extending from it, one pointing to `x` and one pointing to `y`.\nThis implies that `q` causes both `x` and `y`.\nIf `x` were an exposure and `y` and outcome, `q` would be a classic *confounder*.\nIf we were trying to quantify the causal relationship between `x` and `y`, the forking caused by `q` provides a potential *backdoor path* from `x` to `y` that could lead to a spurious estimate of the relationship between `x` and `y` if not accounted for (sometimes we refer to this \"accounting\" as closing the backdoor path).\nAssociations can flow through forks.\nThe second panel of @fig-dag-3 in @fig-dag-3 displays a chain.\nHere, the `x` node has an arrow to the `q` node which in turn has an arrow to the `y` node.\nIf `x` were an exposure and `y` and outcome, `q` would be a classic *mediator*.\nIn the final panel of @fig-dag-3, we see the collider.\nHere `y` has an arrow to `q` and `x` has an arrow to `q`.\nColliders *block* backdoor paths.\nOpposite from confounders, adjusting for a collider can actually *open* a backdoor path.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Three types of paths connecting three nodes, `x`, `y`, and `q`: a fork, a chain, and a collider.](chapter-05_files/figure-html/fig-dag-3-1.png){#fig-dag-3 width=960}\n:::\n:::\n\n\nOften, the goal in causal inference is to quantify the relationship between some exposure and some outcome.\nOne way this estimate can be biased is if we are actually reporting the *correlation* between these two factors rather than the *causal relationship*.\nThinking about these DAGs, spurious correlations can often be attributed to *open backdoor paths*, i.e. the relationship between other factors and our exposure and outcome of interest.\nOne way to think about these paths is to conceptualize associations *flowing* along the paths.\nWithout adjustment, associations flow through forks and chains, and are *blocked* by colliders.\nWith adjustment, the opposite is true, associations are blocked by forks and chains if the node in question is adjusted for, however an association *will* flow through a collider if it is adjusted for.\n\nWhile not strictly necessary, we recommend that DAGs are *time-ordered* from left to right.\nThis helps the analyst ensure that they are not erroneously making assumptions that violate the space-time continuum (for example it is impossible for something in the future to cause something from the past).\n\nHow do we use these in practice?\nThe basic idea is:\n\n1.  Specify your causal question\n2.  Using domain knowledge:\n\n-   Write all relevant variables as *nodes*\n-   Draw causal pathways as arrows (*edges*)\n\n## DAGs in R\n\nLet's begin by specifying our causal question: Does listening to a comedy podcast the morning before an exam improve graduate students test scores?\nWe can diagram this using the method describe in @sec-diag (@fig-podcast).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Does listening to a comedy podcast the morning before an exam improve graduate students test scores?](chapter-05_files/figure-html/fig-podcast-1.png){#fig-podcast width=672}\n:::\n:::\n\n\nThe first step is to specify your DAG.\nIn R, we can use the {ggdag} package for this along with the `dagify()` function.\nThe `dagify()` function takes formulas, separated by commas, that specify cause and effect, with the left element of of the formula specifying the effect and the right all of the factors that cause it.\nWhat are all of the factors that \"cause\" graduate students to listen to a podcast the morning before an exam?\nWhat are all of the factors that could \"cause\" a graduate student to do well on a test?\nLet's posit some here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggdag)\ndagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndag {\nexam\nhumor\nmood\npodcast\nprepared\nhumor -> podcast\nmood -> exam\nmood -> podcast\nprepared -> exam\nprepared -> podcast\n}\n```\n:::\n:::\n\n\nIn the code chunk above, we have posited that a graduate students mood, sense of humor, and how prepared they feel for the exam could influence whether they listened to a podcast the morning of the text.\nLikewise, we posit that their mood and how prepared they are also influences their exam score.\nNotice we *do not* see `podcast` in the `exam` equation -- this means that we assume that there is no causal relationship between podcast and the exam score.\nWe can add additional arguments to `dagify()`, for example, we can time order the coordinates, tag the exposure and outcome, and add labels.\nWe can save this `dagify` object and use the `ggdag()` function to visualize this DAG.\nThis function is a wrapper for a `ggplot2` call, meaning we can add layers like we would to a ggplot object.\nFor example, we can update the theme by adding `+ theme_dag()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npodcast_dag <- dagify(\n  podcast ~ mood + humor + prepared,\n  exam ~ mood + prepared,\n  coords = time_ordered_coords(),\n  exposure = \"podcast\",\n  outcome = \"exam\",\n  labels = c(\n    podcast = \"podcast\",\n    exam = \"exam score\",\n    mood = \"mood\",\n    humor = \"humor\",\n    prepared = \"prepared\"\n  )\n)\nggdag(podcast_dag, use_labels = \"label\", text = FALSE) + \n  theme_dag()\n```\n\n::: {.cell-output-display}\n![Proposed DAG to answer the question: Does listening to a comedy podcast the morning before an exam improve graduate students test scores?](chapter-05_files/figure-html/fig-dag-podcast-1.png){#fig-dag-podcast width=672}\n:::\n:::\n\n\nIn the previous section, we discussed *backdoor paths*.\nThese are paths between factors that could potentially lead us to drawing spurious conclusions about the relationship between our exposure and outcome.\nThe `ggdag_paths()` function will help us identify potential backdoor paths.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggdag_paths(podcast_dag)\n```\n\n::: {.cell-output-display}\n![](chapter-05_files/figure-html/fig-paths-1.png){#fig-paths width=672}\n:::\n:::\n\n\nIn @fig-paths we see two open paths, one through `mood` and one through `prepared`.\nThis tells us we need to find a way to account for these open, non-causal paths.\nSome ways to do this include:\n\n-   Randomization\n-   Stratification, adjustment, weighting, matching, etc.\n\nIn this particular scenario, randomization is likely not possible.\nIt would be challenging to justify randomizing students to listening to a full podcast prior to taking an exam (and it would likely have lots of issues with adherence!).\nStratification is similar to what we demonstrated in @sec-group-sum.\nWe could stratify the students into all possible mood and prepared categories and analyze the causal effect within each stratum -- again, this might pose challenges depending on the sample size and the number of categories we believe exists in each of these factors.\nThe next section will dive into some of these other tools we could use to account for these potential backdoor paths.\n\nThe {ggdag} package can also help us identify adjustment sets.\nIn this particular example, this yields the same result as above, since we need to adjust for both `mood` and `prepared` in order to close the backdoor paths.\nIt is possible, however, to have different ways to close backdoor paths depending on the number of factors and complexity causal relationships assumed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggdag_adjustment_set(\n  podcast_dag,\n  use_labels = \"label\",\n  text = FALSE\n)\n```\n\n::: {.cell-output-display}\n![](chapter-05_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nUsing our proposed DAG, let's simulate some data to see how this might occur in practice!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\nsim_data <- podcast_dag |>\n  simulate_data()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 5\n     exam  humor   mood podcast prepared\n    <dbl>  <dbl>  <dbl>   <dbl>    <dbl>\n 1 -0.435  0.263 -0.100  -0.630   1.07  \n 2 -0.593  0.317  0.143  -1.55    0.0640\n 3  0.786  1.97  -0.591  -0.318  -0.439 \n 4 -0.103  2.86  -0.139   1.07    0.754 \n 5 -0.614 -2.39   0.702   0.464   0.356 \n 6  1.01   1.21   0.910   0.769   0.561 \n 7  0.167 -1.37  -0.559  -0.866   0.214 \n 8  1.16   0.164 -0.743   0.969  -1.67  \n 9  0.650  0.215 -0.248   0.691  -0.303 \n10  0.156  0.713  1.19   -1.02   -0.219 \n# ℹ 490 more rows\n```\n:::\n:::\n\n\nSince we have simulated this data, we know that this is a case where *standard methods will succeed* (see @sec-standard), and therefore can estimate the causal effect using a basic linear regression model.\n@fig-dag-sim shows a forest plot of the simulated data based on our DAG.\nNotice the model that only included the exposure resulted in a spurious effect (an estimate of -0.1 when we know the truth is 0), whereas the model that adjusted for the two variables as suggested by `ggdag_adjustment_set()` is not spurious (0.0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model that does not close backdoor paths\nunadjusted_model <- lm(exam ~ podcast, sim_data) |>\n  broom::tidy(conf.int = TRUE) |>\n  dplyr::filter(term == \"podcast\") |>\n  mutate(formula = \"podcast\")\n\n## Model that closes backdoor paths\nadjusted_model <- lm(exam ~ podcast + mood + prepared, sim_data) |>\n  broom::tidy(conf.int = TRUE) |>\n  dplyr::filter(term == \"podcast\") |>\n  mutate(formula = \"podcast + mood + prepared\")\n\nbind_rows(\n  unadjusted_model,\n  adjusted_model\n) |> \n  ggplot(aes(x = estimate, y = formula, xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, linewidth = 1, color = \"grey80\") + \n  geom_pointrange(fatten = 3, size = 1) +\n  theme_minimal(18) +\n  labs(\n    y = NULL,\n    caption = \"correct effect size: 0\"\n  )\n```\n\n::: {.cell-output-display}\n![Forest plot of simulated data based on the DAG described in @fig-dag-podcast](chapter-05_files/figure-html/fig-dag-sim-1.png){#fig-dag-sim width=672}\n:::\n:::\n\n\n<!-- ## Common Structures of Bias -->\n\n<!-- ## Causal Inference is not (just) a statistical problem {#sec-quartets} -->\n\n<!-- ## Causal and Predictive Models, Revisited {#sec-causal-pred-revisit} -->\n",
    "supporting": [
      "chapter-05_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}